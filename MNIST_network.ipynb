{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.functional import one_hot\n",
    "from torchvision.utils import save_image\n",
    "from preprocessing import batch_elastic_transform\n",
    "from model import PrototypeModel\n",
    "from train import save_images, run_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters for device and reproducibility\n",
    "torch.manual_seed(7)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Globals\n",
    "learning_rate = 0.0001\n",
    "training_epoch = 1500\n",
    "batch_size = 250\n",
    "\n",
    "sigma = 4\n",
    "alpha = 20\n",
    "n_prototypes = 15\n",
    "latent_size = 40\n",
    "n_classes = 10\n",
    "\n",
    "lambda_class = 20\n",
    "lambda_ae = 1\n",
    "lambda_1 = 1              # 1 and 2 here corresponds to the notation we used in the paper\n",
    "lambda_2 = 1\n",
    "\n",
    "model_path = 'models/'\n",
    "prototype_path = 'images/prototypes/'\n",
    "decoding_path = 'images/decoding/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hinriksnaer/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type PrototypeModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/hinriksnaer/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ConvEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/hinriksnaer/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/hinriksnaer/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/hinriksnaer/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sigmoid. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/hinriksnaer/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ConvDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/hinriksnaer/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ConvTranspose2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/hinriksnaer/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type PrototypeClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/hinriksnaer/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  79.56553010940551 Acc:  0.09919999999999989\n",
      "Epoch:  1 Loss:  63.805376068751016 Acc:  0.09929999999999987\n",
      "Epoch:  2 Loss:  58.93859675725301 Acc:  0.09876666666666652\n",
      "Epoch:  3 Loss:  57.97762449582418 Acc:  0.11093333333333333\n",
      "Epoch:  4 Loss:  57.65849153200785 Acc:  0.1122\n",
      "Epoch:  5 Loss:  57.417175404230754 Acc:  0.11213333333333332\n",
      "Epoch:  6 Loss:  57.180316893259686 Acc:  0.1135333333333333\n",
      "Epoch:  7 Loss:  56.852639547983806 Acc:  0.11956666666666659\n",
      "Epoch:  8 Loss:  55.75808108647664 Acc:  0.21913333333333332\n",
      "Epoch:  9 Loss:  51.828138494491576 Acc:  0.3889333333333337\n",
      "Epoch:  10 Loss:  46.827057441075645 Acc:  0.47896666666666715\n",
      "Epoch:  11 Loss:  42.97631827990214 Acc:  0.5275666666666665\n",
      "Epoch:  12 Loss:  40.16992999712626 Acc:  0.5681499999999994\n",
      "Epoch:  13 Loss:  37.93062192598979 Acc:  0.6067999999999996\n",
      "Epoch:  14 Loss:  36.005694301923114 Acc:  0.642333333333333\n",
      "Epoch:  15 Loss:  34.2927796681722 Acc:  0.6699333333333329\n",
      "Epoch:  16 Loss:  32.63585070768992 Acc:  0.6976666666666665\n",
      "Epoch:  17 Loss:  31.191063817342123 Acc:  0.7159833333333333\n",
      "Epoch:  18 Loss:  29.975661738713583 Acc:  0.7331166666666672\n",
      "Epoch:  19 Loss:  28.866591056187946 Acc:  0.7459000000000006\n",
      "Epoch:  20 Loss:  27.772936320304872 Acc:  0.7596000000000002\n",
      "Epoch:  21 Loss:  26.856418307622274 Acc:  0.77015\n",
      "Epoch:  22 Loss:  26.09710052013397 Acc:  0.7801333333333335\n",
      "Epoch:  23 Loss:  25.333283495903014 Acc:  0.789500000000001\n",
      "Epoch:  24 Loss:  24.605157113075258 Acc:  0.7980000000000006\n",
      "Epoch:  25 Loss:  24.00776507059733 Acc:  0.8046166666666674\n",
      "Epoch:  26 Loss:  23.37649393081665 Acc:  0.8103000000000014\n",
      "Epoch:  27 Loss:  22.85024212996165 Acc:  0.8136666666666676\n",
      "Epoch:  28 Loss:  22.20771691004435 Acc:  0.8221500000000007\n",
      "Epoch:  29 Loss:  21.72378519376119 Acc:  0.8258500000000011\n",
      "Epoch:  30 Loss:  21.203478225072224 Acc:  0.832866666666667\n",
      "Epoch:  31 Loss:  20.749704186121622 Acc:  0.8364500000000006\n",
      "Epoch:  32 Loss:  20.33664276997248 Acc:  0.8391833333333342\n",
      "Epoch:  33 Loss:  19.93767535686493 Acc:  0.8434833333333333\n",
      "Epoch:  34 Loss:  19.61505406697591 Acc:  0.8483833333333337\n",
      "Epoch:  35 Loss:  19.333920860290526 Acc:  0.8484000000000014\n",
      "Epoch:  36 Loss:  19.077429382006326 Acc:  0.8514166666666675\n",
      "Epoch:  37 Loss:  18.649390574296316 Acc:  0.8560000000000003\n",
      "Epoch:  38 Loss:  18.37337529261907 Acc:  0.8587499999999999\n",
      "Epoch:  39 Loss:  18.095360227425893 Acc:  0.8610166666666664\n",
      "Epoch:  40 Loss:  17.8317920088768 Acc:  0.8642833333333328\n",
      "Epoch:  41 Loss:  17.580791032314302 Acc:  0.8666333333333333\n",
      "Epoch:  42 Loss:  17.426604640483855 Acc:  0.8671833333333334\n",
      "Epoch:  43 Loss:  17.064800115426383 Acc:  0.8709500000000001\n",
      "Epoch:  44 Loss:  16.86266225973765 Acc:  0.8735833333333332\n",
      "Epoch:  45 Loss:  16.690543393294018 Acc:  0.8746000000000006\n",
      "Epoch:  46 Loss:  16.478887287775674 Acc:  0.8783000000000002\n",
      "Epoch:  47 Loss:  16.227583583196004 Acc:  0.8791666666666664\n",
      "Epoch:  48 Loss:  16.067653254667917 Acc:  0.881133333333333\n",
      "Epoch:  49 Loss:  15.929477365811666 Acc:  0.8818999999999996\n",
      "Epoch:  50 Loss:  15.691937104860942 Acc:  0.8853166666666673\n",
      "Epoch:  51 Loss:  15.518531477451324 Acc:  0.8879499999999998\n",
      "Epoch:  52 Loss:  15.316335586706797 Acc:  0.8886500000000002\n",
      "Epoch:  53 Loss:  15.204792904853822 Acc:  0.8903999999999994\n",
      "Epoch:  54 Loss:  15.062327420711517 Acc:  0.8923166666666662\n",
      "Epoch:  55 Loss:  14.859161333243053 Acc:  0.8931499999999999\n",
      "Epoch:  56 Loss:  14.722988851865132 Acc:  0.8949166666666664\n",
      "Epoch:  57 Loss:  14.577941600481669 Acc:  0.8954666666666655\n",
      "Epoch:  58 Loss:  14.519996392726899 Acc:  0.895133333333333\n",
      "Epoch:  59 Loss:  14.427273960908254 Acc:  0.8977166666666666\n",
      "Epoch:  60 Loss:  14.2777174949646 Acc:  0.8980499999999989\n",
      "Epoch:  61 Loss:  14.093051465352376 Acc:  0.9018666666666656\n",
      "Epoch:  62 Loss:  13.983106489976247 Acc:  0.9022833333333325\n",
      "Epoch:  63 Loss:  13.906832325458527 Acc:  0.9030333333333321\n",
      "Epoch:  64 Loss:  13.761929496129353 Acc:  0.9046833333333325\n",
      "Epoch:  65 Loss:  13.653729979197184 Acc:  0.9065166666666663\n",
      "Epoch:  66 Loss:  13.573731795946758 Acc:  0.906683333333333\n",
      "Epoch:  67 Loss:  13.4770977973938 Acc:  0.9076666666666668\n",
      "Epoch:  68 Loss:  13.407357382774354 Acc:  0.9085333333333324\n",
      "Epoch:  69 Loss:  13.331210641066233 Acc:  0.909849999999999\n",
      "Epoch:  70 Loss:  13.178653113047282 Acc:  0.9102166666666657\n",
      "Epoch:  71 Loss:  13.090377906958262 Acc:  0.9129833333333326\n",
      "Epoch:  72 Loss:  12.984441868464152 Acc:  0.9129166666666658\n",
      "Epoch:  73 Loss:  12.975108746687571 Acc:  0.9116666666666667\n",
      "Epoch:  74 Loss:  12.827360622088115 Acc:  0.9149999999999989\n",
      "Epoch:  75 Loss:  12.76560174226761 Acc:  0.9134999999999995\n",
      "Epoch:  76 Loss:  12.689643681049347 Acc:  0.9142833333333322\n",
      "Epoch:  77 Loss:  12.556228856245676 Acc:  0.9173333333333323\n",
      "Epoch:  78 Loss:  12.488944820562999 Acc:  0.917466666666666\n",
      "Epoch:  79 Loss:  12.40661171277364 Acc:  0.9191666666666652\n",
      "Epoch:  80 Loss:  12.37828921477 Acc:  0.9183833333333321\n",
      "Epoch:  81 Loss:  12.260686147212983 Acc:  0.9205833333333326\n",
      "Epoch:  82 Loss:  12.166800121466318 Acc:  0.9209333333333325\n",
      "Epoch:  83 Loss:  12.100149746735891 Acc:  0.9225499999999993\n",
      "Epoch:  84 Loss:  12.094691967964172 Acc:  0.9208666666666663\n",
      "Epoch:  85 Loss:  12.010139306386312 Acc:  0.921983333333332\n",
      "Epoch:  86 Loss:  11.918949480851492 Acc:  0.9241666666666662\n",
      "Epoch:  87 Loss:  11.826378758748373 Acc:  0.9247333333333329\n",
      "Epoch:  88 Loss:  11.772919734319052 Acc:  0.9242999999999987\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b803965e5efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Get prototypes and decode them to display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/A.I. courses/FACT/FACT-AI/train.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, dataloader, optimizer, iteration, epoch_loss, epoch_accuracy)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Do backward pass and ADAM steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data = MNIST('./data', train=True, download=True, transform=transforms.Compose([\n",
    "                                            transforms.ToTensor(),\n",
    "                                        ]))\n",
    "test_data = MNIST('./data', train=False, download=True,transform=transforms.Compose([\n",
    "                                            transforms.ToTensor(),\n",
    "                                        ]))\n",
    "\n",
    "\n",
    "### Initialize the model and the optimizer.\n",
    "proto = PrototypeModel(15, 40, 10).to(device)\n",
    "optim = torch.optim.Adam(proto.parameters(), lr=learning_rate)\n",
    "dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "# Run for a number of epochs\n",
    "for epoch in range(training_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    it = 0\n",
    "\n",
    "    it, epoch_loss, epoch_acc, dec = run_epoch(proto, dataloader, optim, it, epoch_loss, epoch_acc)\n",
    "\n",
    "    # Get prototypes and decode them to display\n",
    "    prototypes = proto.prototype.get_prototypes()\n",
    "    prototypes = prototypes.view(-1, 10, 2, 2)\n",
    "    imgs = proto.decoder(prototypes)\n",
    "\n",
    "    # Save images\n",
    "    save_images(prototype_path, decoding_path, imgs, dec, epoch)\n",
    "\n",
    "    # Save model\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    torch.save(proto, model_path+\"proto.pth\")\n",
    "\n",
    "    # Print statement to check on progress\n",
    "    print(\"Epoch: \", epoch, \"Loss: \", epoch_loss / it, \"Acc: \", epoch_acc/it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
